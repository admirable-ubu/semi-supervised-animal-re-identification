{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a7b476e",
   "metadata": {},
   "source": [
    "Semisupervised for Restricted Datasets\n",
    "=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc9c064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle as pk\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "from sklearn.base import clone as skclone\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96998aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sslearn.wrapper import TriTraining, WiWTriTraining, DemocraticCoLearning, CoTrainingByCommittee, CoForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eadc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (accuracy_score, \n",
    "                             f1_score, \n",
    "                             hamming_loss, \n",
    "                             jaccard_score,\n",
    "                             precision_score, \n",
    "                             recall_score,\n",
    "                             cohen_kappa_score,\n",
    "                             roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e7314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_scores = {\"acc\":         (accuracy_score,   \"pred\",  {}),\n",
    "#                  \"f1_micro\":        (f1_score,          \"pred\", {\"average\": \"micro\"}),\n",
    "#                  \"f1_macro\":        (f1_score,          \"pred\", {\"average\": \"macro\"}),\n",
    "#                  \"hamming\":         (hamming_loss,      \"pred\", {}),\n",
    "#                  \"jacc_micro\":      (jaccard_score,     \"pred\", {\"average\": \"micro\"}),\n",
    "#                  \"jacc_macro\":      (jaccard_score,     \"pred\", {\"average\": \"macro\"}),\n",
    "#                  \"precision_micro\": (precision_score,   \"pred\", {\"average\": \"micro\"}),\n",
    "#                  \"precision_macro\": (precision_score,   \"pred\", {\"average\": \"macro\"}),\n",
    "#                  \"racall_micro\":    (recall_score,      \"pred\", {\"average\": \"micro\"}),\n",
    "#                  \"racall_macro\":    (recall_score,      \"pred\", {\"average\": \"macro\"}),\n",
    "#                  \"cohen_kappa\":     (cohen_kappa_score, \"pred\", {}) \n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a71c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_done = list(filter(lambda x: \"Test.pkl\" in x, os.listdir(\"results\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fa0c94",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69e255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"files\", \"rb\") as f:\n",
    "    data_pairs = pk.load(f)\n",
    "    \n",
    "datasets = {}\n",
    "    \n",
    "for pair in data_pairs:\n",
    "    \n",
    "    data_identifier = pair[0].split(\"/h2_\")[1].split(\".\")[0]\n",
    "    \n",
    "    if \"MN2\" in data_identifier:\n",
    "        data_first = pd.read_csv(pair[1], header=None)\n",
    "        data_first.columns = [f\"MN2_{i}\" for i in range(1280)]+[\"Labels\"]        \n",
    "        data_second = pd.read_csv(pair[0], header=None)\n",
    "        data_second.columns = [f\"MN2_{i}\" for i in range(1280)]+[\"Labels\"]\n",
    "        \n",
    "        data_first.Labels += 1\n",
    "        data_second.Labels += 1\n",
    "    else:\n",
    "        \n",
    "        data_first = pd.read_csv(pair[1])\n",
    "        data_second = pd.read_csv(pair[0])\n",
    "    \n",
    "    complete_data = pd.concat((data_first, data_second), axis=0, ignore_index=True)\n",
    "    complete_data.Labels = complete_data.Labels.astype(\"int\")\n",
    "    \n",
    "    datasets[data_identifier] = complete_data\n",
    "    \n",
    "# Add frames to MN2\n",
    "for dt in set(map(lambda x: \"_\".join(x.split(\"_\")[:-1]), datasets.keys())):\n",
    "    G = datasets[dt+\"_RGB\"].Frames\n",
    "    datasets[dt+\"_MN2\"][\"Frames\"] = G\n",
    "\n",
    "# Only MN2\n",
    "datasets = dict(filter(lambda x: \"MN2\" in x[0], datasets.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4780a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_dataset(dataframe: pd.DataFrame, unlabel=\"second-half\", ssl=True):\n",
    "    X = dataframe.loc[:, (dataframe.columns != \"Labels\") & (dataframe.columns != \"Frames\")]\n",
    "    y = dataframe.Labels\n",
    "    try:\n",
    "        G = dataframe.Frames\n",
    "    except:\n",
    "        G = False\n",
    "    \n",
    "    y_unlabel = y.copy()\n",
    "    if unlabel == \"first-half\":\n",
    "        condition = G<=G.max()//2       \n",
    "    elif unlabel == \"second-half\":\n",
    "        condition = G>G.max()//2 \n",
    "    else:\n",
    "        return ValueError(f\"unlabel must be 'first-half' or 'second-half', given '{unlabel}'\")\n",
    "    \n",
    "    \n",
    "    U = X[condition]\n",
    "    Uy = y[condition]\n",
    "    Gy = G[condition]\n",
    "    if ssl:\n",
    "        y_unlabel[condition] = -1 \n",
    "    else:\n",
    "        X = X[~condition]\n",
    "        G = G[~condition]\n",
    "        y_unlabel = y[~condition]\n",
    "    \n",
    "    return X, y_unlabel, U, Uy, G, Gy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a206c95a",
   "metadata": {},
   "source": [
    "### Train and score functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2588edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_score(identifier, classifier, ssl=False, wiw=True, test=\"second-half\"):\n",
    "    global datasets, global_scores\n",
    "    \n",
    "    result = {}    \n",
    "    dataset = datasets[identifier]\n",
    "    features = identifier.split(\"_\")[-1]\n",
    "    name = identifier.split(\"_\"+features)[0]  \n",
    "    \n",
    "    X, y, U, Uy, G, Gy = separate_dataset(dataset, test, ssl)\n",
    "    \n",
    "    if wiw:\n",
    "        result[\"clf\"] = skclone(classifier).fit(X, y, G)\n",
    "    else:\n",
    "        result[\"clf\"] = skclone(classifier).fit(X, y)\n",
    "    \n",
    "    result[\"scores\"]= dict()\n",
    "    \n",
    "    if wiw:\n",
    "        y_pred = result[\"clf\"].predict(U, Gy)\n",
    "    else:\n",
    "        y_pred = result[\"clf\"].predict(U)\n",
    "    y_proba = result[\"clf\"].predict_proba(U)\n",
    "    \n",
    "    result[\"pred\"] = y_pred\n",
    "    result[\"proba\"] = y_proba\n",
    "    \n",
    "    for score_name, params in global_scores.items():\n",
    "        score, mode, kwards = params\n",
    "        if mode == \"pred\":\n",
    "            y_score = y_pred\n",
    "        else:\n",
    "            y_score = y_proba\n",
    "\n",
    "        result[\"scores\"][score_name] = score(Uy, y_score, **kwards)\n",
    "        \n",
    "    return name, features, result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c940c84",
   "metadata": {},
   "source": [
    "## Experimenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5f877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experimenter(method, filename, ssl=False, test=\"second-half\", wiw=True, overwrite=False, models=True):\n",
    "    if models:\n",
    "        models = {\"C45\": DecisionTreeClassifier(random_state=0),\n",
    "                  \"5NN\": KNeighborsClassifier(n_jobs=-1), \n",
    "                  \"NB\": GaussianNB(), \n",
    "                  \"LR\": LogisticRegression(random_state=0, n_jobs=-1)}\n",
    "    else:\n",
    "        models = {\"Default\": None}\n",
    "    \n",
    "    resultados = dict()\n",
    "    if overwrite or filename not in experiment_done:\n",
    "        print(\"Calculating:\",filename)\n",
    "        for dataset in datasets:\n",
    "            for model_name, model in models.items():\n",
    "                if model is not None:\n",
    "                    method.set_params(**{\"base_estimator\": model})\n",
    "                name, features, result = fit_score(dataset, method, ssl=ssl, wiw=wiw, test=test)\n",
    "                if name not in resultados:\n",
    "                    resultados[name] = dict()\n",
    "                if features not in resultados[name]:\n",
    "                    resultados[name][features] = dict()\n",
    "                resultados[name][features][model_name] = result\n",
    "\n",
    "\n",
    "        with open(\"results/\"+filename, \"wb\") as f:\n",
    "            pk.dump(resultados, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8556182",
   "metadata": {},
   "source": [
    "### Generate experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95ebf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_base_classifier = {\"DemoCo\": [DecisionTreeClassifier(random_state=0), \n",
    "                                      GaussianNB(),\n",
    "                                      KNeighborsClassifier(n_neighbors=3)],\n",
    "                           \"CoBag\":BaggingClassifier(\n",
    "                               base_estimator=DecisionTreeClassifier(random_state=0),\n",
    "                               random_state=0, n_jobs=-1),\n",
    "                           \"Spreading\": None,\n",
    "                           \"Propagation\": None\n",
    "                          }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470fa9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assigment = [\"Greedy\", \"Hungarian\"]\n",
    "conflict_over = [\"Labeled\", \"Labeled_plus\", \"Unlabeled\", \"All\", \"None\"]\n",
    "test = {\"1Test\": \"first-half\", \"2Test\": \"second-half\"}\n",
    "models = {\"Tri\": TriTraining, \n",
    "          \"DemoCo\": DemocraticCoLearning, \"CoFor\": CoForest, \"CoBag\": CoTrainingByCommittee,\n",
    "          \"Spreading\": LabelSpreading, \"Propagation\": LabelPropagation}\n",
    "experiments = []\n",
    "# Name format: Model - Method - Conflict over - Conflict weighted - Test\n",
    "name = [\"\", \"\", \"\", \"\", \"\"]\n",
    "for model_name, model in models.items():\n",
    "    name[0] = model_name\n",
    "    base_experiment = {\"method\": None, \"filename\": \"\", \"ssl\": False, \"test\": \"\", \"wiw\": True}\n",
    "    \n",
    "    if \"WiW\" not in model_name:\n",
    "        base_experiment[\"wiw\"] = False\n",
    "    if \"Tri\" in model_name or model_name != \"WiW\":\n",
    "        base_experiment[\"ssl\"] = True\n",
    "        \n",
    "    for test_name, test_type in test.items():\n",
    "        exp = base_experiment.copy()\n",
    "        exp[\"test\"] = test_type\n",
    "        name[4] = test_name\n",
    "        if exp[\"wiw\"]:\n",
    "            for ass in assigment:\n",
    "                name[1] = ass\n",
    "                kwards = {\"method\": ass.lower()}\n",
    "                if model_name == \"WiWTri\":\n",
    "                    for co in conflict_over:\n",
    "                        kwards[\"conflict_over\"] = co.lower()\n",
    "                        name[2] = co\n",
    "                        if co != \"None\":\n",
    "                            for weighted in [True, False]:\n",
    "                                kwards[\"conflict_weighted\"] = weighted\n",
    "                                if weighted:\n",
    "                                    name[3] = \"Weighted\"\n",
    "                                else:\n",
    "                                    name[3] = \"NoWeighted\"\n",
    "                            \n",
    "                                exp_end = exp.copy()\n",
    "                                exp_end[\"method\"] = model(base_estimator=DummyClassifier(), random_state=0, **kwards)\n",
    "                                exp_end[\"filename\"] = \"-\".join(name)+\".pkl\"\n",
    "                                experiments.append(exp_end)\n",
    "                        else:\n",
    "                            name[3] = \"\"\n",
    "                            exp_end = exp.copy()\n",
    "                            exp_end[\"method\"] = model(base_estimator=DummyClassifier(), random_state=0, **kwards)\n",
    "                            exp_end[\"filename\"] = \"-\".join(name)+\".pkl\"\n",
    "                            experiments.append(exp_end)\n",
    "                else:\n",
    "                    name[2] = \"\"\n",
    "                    name[3] = \"\"\n",
    "                    exp_end = exp.copy()\n",
    "                    exp_end[\"method\"] = model(base_estimator=DummyClassifier(), **kwards)\n",
    "                    exp_end[\"filename\"] = \"-\".join(name)+\".pkl\"\n",
    "                    experiments.append(exp_end)\n",
    "        else:\n",
    "            name[1] = \"\"\n",
    "            name[2] = \"\"\n",
    "            name[3] = \"\"\n",
    "            exp_end = exp.copy()\n",
    "            if model_name in special_base_classifier:\n",
    "                if special_base_classifier[model_name] is not None:\n",
    "                    exp_end[\"method\"] = model(special_base_classifier[model_name], random_state=0)\n",
    "                else:\n",
    "                    exp_end[\"method\"] = model()\n",
    "            else:\n",
    "                exp_end[\"method\"] = model(base_estimator=DummyClassifier(), random_state=0)\n",
    "            exp_end[\"filename\"] = \"-\".join(name)+\".pkl\"\n",
    "            experiments.append(exp_end)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c10675",
   "metadata": {},
   "source": [
    "### Launch experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df91235",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for exp in experiments:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        models=True\n",
    "        if exp[\"filename\"].split(\"-\")[0] in special_base_classifier:\n",
    "            models=False\n",
    "        exp[\"filename\"]=\"MN2\"+exp[\"filename\"]\n",
    "        experimenter(overwrite=False, models=models, **exp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sslearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "58c69c02a72c3ad66bcc5b78056b9817c4200bc39c2b1eaeb4d99ed460b41b7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
